{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, LabelEncoder\n",
    "from keras.layers import Input, Embedding, Dense, Flatten, Dropout, SpatialDropout1D, Activation, concatenate\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.layers.advanced_activations import ReLU, PReLU, LeakyReLU, ELU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "\n",
    "COLUMNS = [\n",
    "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\", \"marital_status\", \n",
    "    \"occupation\", \"relationship\", \"race\", \"gender\", \"capital_gain\", \"capital_loss\", \n",
    "    \"hours_per_week\", \"native_country\", \"income_bracket\"\n",
    "]\n",
    "\n",
    "LABEL_COLUMN = \"label\"\n",
    "\n",
    "CATEGORICAL_COLUMNS = [\n",
    "    \"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\", \n",
    "    \"race\", \"gender\", \"native_country\"\n",
    "] #非数字的列，分类数据\n",
    "\n",
    "CONTINUOUS_COLUMNS = [\n",
    "    \"age\", \"education_num\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"\n",
    "] #连续数据对应的列\n",
    "\n",
    "\n",
    "def preprocessing(): #数据集处理\n",
    "    train_data = pd.read_csv('./adult.data', names=COLUMNS) #打开adult.data训练数据集，列名是COLUMNS\n",
    "    train_data.dropna(how='any', axis=0) #删除带有空值的行，只要有一个空值，就删除整行\n",
    "    test_data = pd.read_csv('./adult.test', skiprows=1, names=COLUMNS) #打开adult.test测试数据集\n",
    "    test_data.dropna(how='any', axis=0) #删除带有空值的行，只要有一个空值，就删除整行\n",
    "    all_data = pd.concat([train_data, test_data]) #将训练集和测试集拼接在一起\n",
    "    # 数値化\n",
    "    all_data[LABEL_COLUMN] = all_data['income_bracket'].apply(lambda x: \">50K\" in x).astype(int) #添加一列标签，收入<=50K令其为0，>50K令其为1\n",
    "    all_data.pop('income_bracket') #将原收入那一列删除\n",
    "    y = all_data[LABEL_COLUMN].values #y为标签\n",
    "    all_data.pop(LABEL_COLUMN) #处理完后进行删除\n",
    "    for c in CATEGORICAL_COLUMNS:\n",
    "        le = LabelEncoder() #使用sklearn中的LabelEncoder()进行编码\n",
    "        all_data[c] = le.fit_transform(all_data[c])\n",
    "    train_size = len(train_data) #获取训练集的大小\n",
    "    x_train = all_data.iloc[:train_size] #x_train表示前train_size个数据是训练集\n",
    "    y_train = y[:train_size] #y_train表示y中前train_size个数据是训练集对应的标签\n",
    "    x_test = all_data.iloc[train_size:] #测试集数据\n",
    "    y_test = y[train_size:] #测试集标签\n",
    "    x_train_categ = np.array(x_train[CATEGORICAL_COLUMNS]) #训练集中的类别数据，转成numpy类型\n",
    "    x_test_categ = np.array(x_test[CATEGORICAL_COLUMNS]) #测试集中的类别类别数据\n",
    "    x_train_conti = np.array(x_train[CONTINUOUS_COLUMNS], dtype='float64') # 训练集中的连续数据\n",
    "    x_test_conti = np.array(x_test[CONTINUOUS_COLUMNS], dtype='float64') #测试集中的连续数据，\n",
    "    scaler = StandardScaler()\n",
    "    x_train_conti = scaler.fit_transform(x_train_conti) # 连续训练数据标准化\n",
    "    x_test_conti = scaler.transform(x_test_conti)\n",
    "    return [x_train, y_train, x_test, y_test, x_train_categ, x_test_categ, x_train_conti, x_test_conti, all_data]\n",
    "\n",
    "\n",
    "class Wide_and_Deep:\n",
    "    def __init__(self, mode='wide and deep'):\n",
    "        self.mode = mode\n",
    "        x_train, y_train, x_test, y_test, x_train_categ, x_test_categ, x_train_conti, x_test_conti, all_data \\\n",
    "            = preprocessing()\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        self.x_train_categ = x_train_categ \n",
    "        self.x_test_categ = x_test_categ \n",
    "        self.x_train_conti = x_train_conti \n",
    "        self.x_test_conti = x_test_conti \n",
    "        self.all_data = all_data\n",
    "        self.poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "        # 将分类数据进行cross product化\n",
    "        self.x_train_categ_poly = self.poly.fit_transform(x_train_categ)\n",
    "        self.x_test_categ_poly = self.poly.transform(x_test_categ)\n",
    "        self.categ_inputs = None\n",
    "        self.conti_input = None\n",
    "        self.deep_component_outlayer = None\n",
    "        self.logistic_input = None\n",
    "        self.model = None\n",
    "\n",
    "    def deep_component(self):\n",
    "        categ_inputs = []\n",
    "        categ_embeds = []\n",
    "        # 按分类数据的特征分别创建Input层和Embeding层\n",
    "        for i in range(len(CATEGORICAL_COLUMNS)):\n",
    "            input_i = Input(shape=(1,), dtype='int32')\n",
    "            dim = len(np.unique(self.all_data[CATEGORICAL_COLUMNS[i]]))\n",
    "            embed_dim = int(np.ceil(dim ** 0.25)) \n",
    "            embed_i = Embedding(dim, embed_dim, input_length=1)(input_i)\n",
    "            flatten_i = Flatten()(embed_i)\n",
    "            categ_inputs.append(input_i)\n",
    "            categ_embeds.append(flatten_i)\n",
    "        # 在所有结合层中统一输入连续数据\n",
    "        conti_input = Input(shape=(len(CONTINUOUS_COLUMNS),))\n",
    "        conti_dense = Dense(256, use_bias=False)(conti_input)\n",
    "        # 将所有结合层和每个Embeding的输出相结合\n",
    "        concat_embeds = concatenate([conti_dense]+categ_embeds)\n",
    "        concat_embeds = Activation('relu')(concat_embeds)\n",
    "        bn_concat = BatchNormalization()(concat_embeds)\n",
    "        # 进一步将所有结合层重叠3层\n",
    "        fc1 = Dense(512, use_bias=False)(bn_concat)\n",
    "        ac1 = ReLU()(fc1)\n",
    "        bn1 = BatchNormalization()(ac1)\n",
    "        fc2 = Dense(256, use_bias=False)(bn1)\n",
    "        ac2 = ReLU()(fc2)\n",
    "        bn2 = BatchNormalization()(ac2)\n",
    "        fc3 = Dense(128)(bn2)\n",
    "        ac3 = ReLU()(fc3)\n",
    "\n",
    "        # 将输入层和最后一层进行成员变量化（用于制作模型）\n",
    "        self.categ_inputs = categ_inputs\n",
    "        self.conti_input = conti_input\n",
    "        self.deep_component_outlayer = ac3\n",
    "\n",
    "    def wide_component(self):\n",
    "        # 只将分类数据列入线性模型\n",
    "        dim = self.x_train_categ_poly.shape[1]\n",
    "        self.logistic_input = Input(shape=(dim,))\n",
    "\n",
    "    def create_model(self):\n",
    "        self.deep_component()\n",
    "        self.wide_component()\n",
    "        if self.mode == 'wide and deep':\n",
    "            out_layer = concatenate([self.deep_component_outlayer, self.logistic_input])\n",
    "            inputs = [self.conti_input] + self.categ_inputs + [self.logistic_input]\n",
    "        elif self.mode =='deep':\n",
    "            out_layer = self.deep_component_outlayer\n",
    "            inputs = [self.conti_input] + self.categ_inputs\n",
    "        else:\n",
    "            print('wrong mode')\n",
    "            return\n",
    "\n",
    "        output = Dense(1, activation='sigmoid')(out_layer) #sigmoid函数作为最后一层的输出激活函数\n",
    "        self.model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    def train_model(self, epochs=15, optimizer='adam', batch_size=128):\n",
    "        if not self.model:\n",
    "            print('You have to create model first')\n",
    "            return\n",
    "\n",
    "        if self.mode == 'wide and deep':\n",
    "            input_data = [self.x_train_conti] +\\\n",
    "                         [self.x_train_categ[:, i] for i in range(self.x_train_categ.shape[1])] +\\\n",
    "                         [self.x_train_categ_poly]\n",
    "        elif self.mode == 'deep':\n",
    "            input_data = [self.x_train_conti] +\\\n",
    "                         [self.x_train_categ[:, i] for i in range(self.x_train_categ.shape[1])]\n",
    "        else:\n",
    "            print('wrong mode')\n",
    "            return\n",
    "        \n",
    "        self.model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        self.model.fit(input_data, self.y_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        if not self.model:\n",
    "            print('You have to create model first')\n",
    "            return\n",
    "\n",
    "        if self.mode == 'wide and deep':\n",
    "            input_data = [self.x_test_conti] +\\\n",
    "                         [self.x_test_categ[:, i] for i in range(self.x_test_categ.shape[1])] +\\\n",
    "                         [self.x_test_categ_poly]\n",
    "        elif self.mode == 'deep':\n",
    "            input_data = [self.x_test_conti] +\\\n",
    "                         [self.x_test_categ[:, i] for i in range(self.x_test_categ.shape[1])]\n",
    "        else:\n",
    "            print('wrong mode')\n",
    "            return\n",
    "\n",
    "        loss, acc = self.model.evaluate(input_data, self.y_test)\n",
    "        print(f'test_loss: {loss} - test_acc: {acc}')\n",
    "\n",
    "    def save_model(self, filename='wide_and_deep.h5'):\n",
    "        self.model.save(filename)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    wide_deep_net = Wide_and_Deep()\n",
    "    wide_deep_net.create_model()\n",
    "    wide_deep_net.train_model()\n",
    "    wide_deep_net.evaluate_model()\n",
    "    wide_deep_net.save_model()\n",
    "    plot_model(wide_deep_net.model, to_file='model.png', show_shapes=True, show_layer_names=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
